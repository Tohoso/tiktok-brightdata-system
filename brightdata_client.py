#!/usr/bin/env python3
"""
Bright Data TikTok Scraper API Client
24æ™‚é–“ä»¥å†…ãƒ»50ä¸‡å†ç”Ÿä»¥ä¸Šã®å‹•ç”»ã‚’åŠ¹ç‡çš„ã«åé›†
"""

import requests
import json
import time
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import uuid

class BrightDataClient:
    """Bright Data TikTok Scraper APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, api_key: str, dataset_id: str, timeout: int = 300):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: Bright Data APIã‚­ãƒ¼
            dataset_id: TikTokã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.dataset_id = dataset_id
        self.timeout = timeout
        self.base_url = "https://api.brightdata.com/dca"
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        })
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = logging.getLogger(__name__)
    
    def trigger_scraping_job(self, urls: List[str], 
                           country: str = "JP",
                           additional_params: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹
        
        Args:
            urls: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã®URLä¸€è¦§
            country: å¯¾è±¡å›½ï¼ˆJP=æ—¥æœ¬ï¼‰
            additional_params: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ã‚¸ãƒ§ãƒ–æƒ…å ±ï¼ˆsnapshot_idã‚’å«ã‚€ï¼‰
        """
        try:
            # ã‚¸ãƒ§ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹ç¯‰
            job_params = {
                "dataset_id": self.dataset_id,
                "country": country,
                "urls": urls,
                "format": "json",
                "webhook_notification_url": None,
                "notify_on": ["success", "error"]
            }
            
            # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
            if additional_params:
                job_params.update(additional_params)
            
            self.logger.info(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–é–‹å§‹: {len(urls)}ä»¶ã®URL")
            
            # APIå‘¼ã³å‡ºã—
            response = self.session.post(
                f"{self.base_url}/trigger",
                json=job_params,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            result = response.json()
            
            self.logger.info(f"ã‚¸ãƒ§ãƒ–é–‹å§‹æˆåŠŸ: snapshot_id={result.get('snapshot_id')}")
            return result
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¸ãƒ§ãƒ–é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            self.logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def get_job_status(self, snapshot_id: str) -> Dict[str, Any]:
        """
        ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—
        
        Args:
            snapshot_id: ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆID
            
        Returns:
            ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±
        """
        try:
            response = self.session.get(
                f"{self.base_url}/get_snapshot_status",
                params={"snapshot_id": snapshot_id},
                timeout=self.timeout
            )
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def wait_for_completion(self, snapshot_id: str, 
                          max_wait_time: int = 1800,
                          check_interval: int = 30) -> List[Dict[str, Any]]:
        """
        ã‚¸ãƒ§ãƒ–å®Œäº†ã¾ã§å¾…æ©Ÿã—ã€çµæœã‚’å–å¾—
        
        Args:
            snapshot_id: ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆID
            max_wait_time: æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
            check_interval: ãƒã‚§ãƒƒã‚¯é–“éš”ï¼ˆç§’ï¼‰
            
        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ãƒªã‚¹ãƒˆ
        """
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            try:
                status = self.get_job_status(snapshot_id)
                job_status = status.get('status', 'unknown')
                
                self.logger.info(f"ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {job_status}")
                
                if job_status == 'completed':
                    self.logger.info("ã‚¸ãƒ§ãƒ–å®Œäº†ã€çµæœã‚’å–å¾—ä¸­...")
                    return self.get_results(snapshot_id)
                elif job_status == 'failed':
                    error_msg = status.get('error', 'Unknown error')
                    raise Exception(f"ã‚¸ãƒ§ãƒ–å¤±æ•—: {error_msg}")
                elif job_status in ['running', 'pending']:
                    self.logger.info(f"ã‚¸ãƒ§ãƒ–å®Ÿè¡Œä¸­... {check_interval}ç§’å¾Œã«å†ãƒã‚§ãƒƒã‚¯")
                    time.sleep(check_interval)
                else:
                    self.logger.warning(f"ä¸æ˜ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {job_status}")
                    time.sleep(check_interval)
                    
            except Exception as e:
                self.logger.error(f"ã‚¸ãƒ§ãƒ–ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(check_interval)
        
        raise TimeoutError(f"ã‚¸ãƒ§ãƒ–ãŒ{max_wait_time}ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
    
    def get_results(self, snapshot_id: str) -> List[Dict[str, Any]]:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’å–å¾—
        
        Args:
            snapshot_id: ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆID
            
        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            response = self.session.get(
                f"{self.base_url}/get_snapshot_data",
                params={
                    "snapshot_id": snapshot_id,
                    "format": "json"
                },
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒJSONã®å ´åˆã¨NDJSONã®å ´åˆã‚’å‡¦ç†
            content_type = response.headers.get('content-type', '')
            
            if 'application/json' in content_type:
                return response.json()
            else:
                # NDJSONå½¢å¼ã®å ´åˆ
                results = []
                for line in response.text.strip().split('\n'):
                    if line.strip():
                        results.append(json.loads(line))
                return results
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"çµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def scrape_tiktok_discover_pages(self, country: str = "JP") -> List[Dict[str, Any]]:
        """
        TikTokç™ºè¦‹ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            country: å¯¾è±¡å›½ã‚³ãƒ¼ãƒ‰
            
        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ
        """
        # æ—¥æœ¬ã®TikTokç™ºè¦‹ãƒšãƒ¼ã‚¸URL
        discover_urls = [
            "https://www.tiktok.com/discover",
            "https://www.tiktok.com/trending",
            "https://www.tiktok.com/foryou",
            "https://www.tiktok.com/explore"
        ]
        
        self.logger.info(f"TikTokç™ºè¦‹ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {country}")
        
        # ã‚¸ãƒ§ãƒ–é–‹å§‹
        job_result = self.trigger_scraping_job(
            urls=discover_urls,
            country=country,
            additional_params={
                "include_posts": True,
                "max_posts_per_page": 100
            }
        )
        
        snapshot_id = job_result.get('snapshot_id')
        if not snapshot_id:
            raise Exception("snapshot_idãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # çµæœå¾…æ©Ÿãƒ»å–å¾—
        return self.wait_for_completion(snapshot_id)
    
    def scrape_hashtag_posts(self, hashtags: List[str], 
                           country: str = "JP") -> List[Dict[str, Any]]:
        """
        ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ•ç¨¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        
        Args:
            hashtags: ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒªã‚¹ãƒˆ
            country: å¯¾è±¡å›½ã‚³ãƒ¼ãƒ‰
            
        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ
        """
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°URLã‚’æ§‹ç¯‰
        hashtag_urls = [
            f"https://www.tiktok.com/tag/{hashtag.lstrip('#')}"
            for hashtag in hashtags
        ]
        
        self.logger.info(f"ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ•ç¨¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {len(hashtags)}ä»¶")
        
        # ã‚¸ãƒ§ãƒ–é–‹å§‹
        job_result = self.trigger_scraping_job(
            urls=hashtag_urls,
            country=country,
            additional_params={
                "include_posts": True,
                "max_posts_per_hashtag": 50
            }
        )
        
        snapshot_id = job_result.get('snapshot_id')
        if not snapshot_id:
            raise Exception("snapshot_idãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        # çµæœå¾…æ©Ÿãƒ»å–å¾—
        return self.wait_for_completion(snapshot_id)
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """
        APIä½¿ç”¨çµ±è¨ˆã‚’å–å¾—
        
        Returns:
            ä½¿ç”¨çµ±è¨ˆæƒ…å ±
        """
        try:
            response = self.session.get(
                f"{self.base_url}/get_usage_stats",
                timeout=self.timeout
            )
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"ä½¿ç”¨çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise


def test_brightdata_client():
    """Bright Data ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ"""
    import os
    
    # ãƒ†ã‚¹ãƒˆç”¨è¨­å®šï¼ˆå®Ÿéš›ã®APIã‚­ãƒ¼ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    api_key = os.getenv('BRIGHT_DATA_API_KEY', 'test_key')
    dataset_id = "gd_l7q7dkf244hwjntr0"  # TikTokã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID
    
    client = BrightDataClient(api_key, dataset_id)
    
    print("Bright Data TikTokã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ¢ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®APIã‚­ãƒ¼ãŒãªã„å ´åˆï¼‰
    if api_key == 'test_key':
        print("âš ï¸  å®Ÿéš›ã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    try:
        # ä½¿ç”¨çµ±è¨ˆå–å¾—ãƒ†ã‚¹ãƒˆ
        print("ğŸ“Š ä½¿ç”¨çµ±è¨ˆå–å¾—ãƒ†ã‚¹ãƒˆ...")
        stats = client.get_usage_stats()
        print(f"âœ… ä½¿ç”¨çµ±è¨ˆå–å¾—æˆåŠŸ: {stats}")
        
        # å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ
        print("\nğŸ” å°è¦æ¨¡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ...")
        test_urls = ["https://www.tiktok.com/discover"]
        
        job_result = client.trigger_scraping_job(test_urls)
        print(f"âœ… ã‚¸ãƒ§ãƒ–é–‹å§‹æˆåŠŸ: {job_result}")
        
        # çµæœå–å¾—ï¼ˆçŸ­æ™‚é–“ã§å®Œäº†ã™ã‚‹å ´åˆã®ã¿ï¼‰
        snapshot_id = job_result.get('snapshot_id')
        if snapshot_id:
            print(f"ğŸ“‹ ã‚¸ãƒ§ãƒ–ç›£è¦–ä¸­: {snapshot_id}")
            # å®Ÿéš›ã®é‹ç”¨ã§ã¯ wait_for_completion ã‚’ä½¿ç”¨
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    test_brightdata_client()

